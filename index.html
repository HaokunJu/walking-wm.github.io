<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Walking World Model for Visually Impaired Path Following">
  <!-- 简短描述，方便之后自己再改 -->
  <meta name="description" content="We present a predictive path-following approach for visually impaired navigation that uses a learned walking world model and MPC to issue proactive vibrotactile guidance, improving safety, walking speed, and cognitive load compared to reactive baselines.">
  <meta name="keywords" content="visually impaired navigation, vibrotactile feedback, walking world model, model predictive control, human-centered robotics, assistive navigation">
  <meta name="author" content="Haokun Ju, Lixuan Zhang, Xiangyu Cao, Meina Kan, Shiguang Shan, Xilin Chen">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="State Key Laboratory of AI Safety, ICT, CAS">
  <meta property="og:title" content="Walking World Model for Visually Impaired Path Following">
  <meta property="og:description" content="A predictive path-following approach for visually impaired users based on a walking world model and MPC with cognitive-load-aware guidance.">
  <!-- TODO: 把下面这个 URL 换成你 GitHub Pages 实际地址 -->
  <meta property="og:url" content="https://YOUR_USERNAME.github.io/walking-world-model">
  <!-- TODO: 换成你自己的预览图路径 -->
  <meta property="og:image" content="https://YOUR_USERNAME.github.io/walking-world-model/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Walking World Model for Visually Impaired Path Following - teaser image">
  <meta property="article:published_time" content="2025-01-01T00:00:00.000Z">
  <meta property="article:author" content="Haokun Ju">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="visually impaired navigation">
  <meta property="article:tag" content="model predictive control">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: 替换成你们实验室 / 个人 Twitter -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <meta name="twitter:title" content="Walking World Model for Visually Impaired Path Following">
  <meta name="twitter:description" content="Predictive vibrotactile guidance for visually impaired path following using a walking world model and MPC.">
  <meta name="twitter:image" content="https://YOUR_USERNAME.github.io/walking-world-model/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="Walking World Model for Visually Impaired Path Following - teaser image">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Walking World Model for Visually Impaired Path Following">
  <meta name="citation_author" content="Ju, Haokun">
  <meta name="citation_author" content="Zhang, Lixuan">
  <meta name="citation_author" content="Cao, Xiangyu">
  <meta name="citation_author" content="Kan, Meina">
  <meta name="citation_author" content="Shan, Shiguang">
  <meta name="citation_author" content="Chen, Xilin">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_journal_title" content="IEEE Robotics and Automation Letters">
  <!-- TODO: 换成你最终 PDF 的在线地址 -->
  <meta name="citation_pdf_url" content="https://YOUR_USERNAME.github.io/walking-world-model/static/pdfs/walking_world_model.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>Walking World Model for Visually Impaired Path Following | Academic Project Page</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Walking World Model for Visually Impaired Path Following",
    "description": "A predictive path-following approach for visually impaired navigation that uses a walking world model and model predictive control with cognitive-load-aware guidance.",
    "author": [
      {
        "@type": "Person",
        "name": "Haokun Ju",
        "affiliation": {
          "@type": "Organization",
          "name": "State Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences"
        }
      },
      {
        "@type": "Person",
        "name": "Lixuan Zhang",
        "affiliation": {
          "@type": "Organization",
          "name": "State Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences"
        }
      },
      {
        "@type": "Person",
        "name": "Xiangyu Cao",
        "affiliation": {
          "@type": "Organization",
          "name": "State Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences"
        }
      },
      {
        "@type": "Person",
        "name": "Meina Kan",
        "affiliation": {
          "@type": "Organization",
          "name": "State Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences"
        }
      },
      {
        "@type": "Person",
        "name": "Shiguang Shan",
        "affiliation": {
          "@type": "Organization",
          "name": "State Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences"
        }
      },
      {
        "@type": "Person",
        "name": "Xilin Chen",
        "affiliation": {
          "@type": "Organization",
          "name": "State Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences"
        }
      }
    ],
    "datePublished": "2025-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "IEEE Robotics and Automation Letters"
    },
    "url": "https://YOUR_USERNAME.github.io/walking-world-model",
    "image": "https://YOUR_USERNAME.github.io/walking-world-model/static/images/social_preview.png",
    "keywords": ["visually impaired navigation", "vibrotactile feedback", "model predictive control", "world model", "assistive robotics"],
    "abstract": "Guiding visually impaired individuals (VI) walking along planned paths is essential for enabling independent long-distance mobility. Current reactive approaches only correct deviations after they occur, ignoring VI’s walking dynamics such as reaction latency and heading drift, which causes frequent interventions, increased cognitive load, and missed turns. We propose a predictive path-following approach enhanced by a walking world model that predicts users’ future states under given vibrotactile commands. To efficiently obtain training data, we adopt a two-stage scheme that first self-supervises on large-scale free-walking data to learn general gait patterns and then fine-tunes on action-annotated data to model instruction-conditioned dynamics. Integrated into a model predictive control framework with an explicit cognitive load cost, our method proactively optimizes guidance commands to minimize deviation, ensure safety, and reduce cognitive load. Real-world experiments with visually impaired and eye-masked participants demonstrate improved walking speed, reduced collisions, and lower cognitive workload compared to reactive baselines.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_USERNAME.github.io/walking-world-model"
    }
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "State Key Laboratory of AI Safety, ICT, CAS",
    "url": "https://www.ict.ac.cn",
    "logo": "https://YOUR_USERNAME.github.io/walking-world-model/static/images/favicon.ico",
    "sameAs": [
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- TODO: 替换为你们实验室的相关工作链接 -->
        <a href="https://arxiv.org/abs/PAPER_ID_1" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 1</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 2</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Walking World Model for Visually Impaired Path Following
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: 如果之后有个人主页，把 # 换成真实链接 -->
              <span class="author-block">
                <a href="#" target="_blank">Haokun Ju</a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Lixuan Zhang</a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Xiangyu Cao</a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Meina Kan</a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Shiguang Shan</a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Xilin Chen</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                State Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences<br>
                University of Chinese Academy of Sciences<br>
                IEEE Robotics and Automation Letters (RA-L), 2025
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <!-- TODO: 把 PDF 路径换成你自己放的论文 PDF -->
                  <a href="static/pdfs/walking_world_model.pdf" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- 如果暂时没有 Supplementary，可以先注释掉这个块 -->


                <span class="link-block">
                  <!-- TODO: 如果有 arXiv，填上对应 ID；没有的话可以删掉这个按钮 -->
                  <a href="https://arxiv.org/abs/XXXX.XXXXX" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      TODO: 替换为你自己的 teaser 视频 -->
      <!-- <iframe src="static/images/Hardware.pdf" width="100%" height="610pt"></iframe>
      <h2 class="subtitle has-text-centered">
        Our navigation system combines a wearable RGB-D perception module, a haptic belt, and a walking world model within an MPC loop to proactively guide visually impaired users along planned paths.
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Guiding visually impaired individuals (VI) walking along planned paths is essential for enabling independent long-distance
            mobility. Current reactive approaches only correct deviations after they occur. These methods ignore VI’s walking dynamics
            (e.g., reaction latency and heading drift), resulting in frequent interventions that increase cognitive load, reduce walking
            efficiency, and may lead to missed turns. To address these limitations, we propose a predictive path-following approach
            enhanced by a walking world model to enable proactive guidance through vibrotactile guidance commands. Specifically, our
            walking world model is used to predict the future state of users after receiving specific commands. To mitigate the
            inefficiency in collecting action-annotated walking data, we exploit unannotated free-walking data to enhance model
            generalization. The model first undergoes self-supervised pre-training on a large unannotated dataset to learn general gait
            patterns, and is then fine-tuned on annotated data with action labels to model the walking dynamics of users given guidance
            commands. Integrated with model predictive control (MPC) specially considering cognitive load for the human, our method
            proactively optimizes instructions to minimize deviation, ensure safety, and reduce cognitive load. Experiments show
            significant improvements in walking speed and cognitive load over reactive baselines.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
<!-- Begin Motivation-->
 <!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Guiding visually impaired individuals (VI) walking along planned paths is essential for enabling independent long-distance
            mobility. Current reactive approaches only correct deviations after they occur. These methods ignore VI’s walking dynamics
            (e.g., reaction latency and heading drift), resulting in frequent interventions that increase cognitive load, reduce walking
            efficiency, and may lead to missed turns. To address these limitations, we propose a predictive path-following approach
            enhanced by a walking world model to enable proactive guidance through vibrotactile guidance commands. Specifically, our
            walking world model is used to predict the future state of users after receiving specific commands. To mitigate the
            inefficiency in collecting action-annotated walking data, we exploit unannotated free-walking data to enhance model
            generalization. The model first undergoes self-supervised pre-training on a large unannotated dataset to learn general gait
            patterns, and is then fine-tuned on annotated data with action labels to model the walking dynamics of users given guidance
            commands. Integrated with model predictive control (MPC) specially considering cognitive load for the human, our method
            proactively optimizes instructions to minimize deviation, ensure safety, and reduce cognitive load. Experiments show
            significant improvements in walking speed and cognitive load over reactive baselines.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
 <!--End Motivation-->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <h2 class="title is-3 has-text-centered">Method</h2>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <!-- Item 1: Training paradigm (pre-training + fine-tuning) -->
        <div class="item">
          <img src="static/images/training paradigm.png"
               alt="Training paradigm"
               loading="lazy" />
          <h2 class="subtitle has-text-centered">
            Training paradigm of the Walking-WM.
          </h2>

          <!-- Pre-training and fine-tuning text -->
          <p class="content is-small">
            <strong>Pre-training from action-free walking data.</strong>
            Given an action-free walking sequence
            <em>s</em><sub>1</sub>, <em>s</em><sub>2</sub>, …, <em>s</em><sub>T</sub>,
            a single-layer bidirectional GRU <em>g</em> is used as an action
            extractor to infer latent actions between adjacent states:
            {<em>a</em><sup>*</sup><sub>1</sub>, …, <em>a</em><sup>*</sup><sub>T−1</sub>} =
            <em>g</em>(<em>s</em><sub>1</sub>, …, <em>s</em><sub>T</sub>).
            These latent actions drive a recurrent state-space world model
            composed of a recurrent model, a latent transition model,
            a state decoder and a representation model:
            <em>h</em><sub>t+1</sub> = <em>r</em>(<em>h</em><sub>t</sub>, <em>z</em><sub>t</sub>, <em>a</em><sup>*</sup><sub>t</sub>),
            <em>ẑ</em><sub>t+1</sub> = <em>h</em>(<em>h</em><sub>t+1</sub>),
            <em>ŝ</em><sub>t+1</sub> = <em>d</em>(<em>ẑ</em><sub>t+1</sub>),
            <em>z</em><sub>t+1</sub> = <em>q</em>(<em>h</em><sub>t+1</sub>, <em>s</em><sub>t+1</sub>).
            All components are trained jointly with a reconstruction and
            consistency loss
            <em>L</em><sub>pre-train</sub> =
            &#8741;<em>ẑ</em><sub>t</sub> − <em>z</em><sub>t</sub>&#8741;<sup>2</sup>
            + &alpha;&#8741;<em>s</em><sub>t</sub> − <em>d</em>(<em>z</em><sub>t</sub>)&#8741;<sup>2</sup>.
            Here, the first term encourages consistent latent dynamics, and
            the second term encourages accurate reconstruction of observable
            states.
          </p>

          <p class="content is-small">
            <strong>Fine-tuning with action-annotated walking data.</strong>
            After pre-training, the world model is fine-tuned on trajectories
            with explicit guidance commands <em>a</em><sub>t</sub>.
            To reuse the latent dynamics learned from free walking,
            an action adapter <em>f</em> (a 2-layer MLP) maps each
            state–action pair into the same latent action space:
            <em>ã</em><sub>t</sub> = <em>f</em>(<em>s</em><sub>t</sub>, <em>a</em><sub>t</sub>).
            The world model is then updated as
            <em>h</em><sub>t+1</sub> = <em>r</em>(<em>h</em><sub>t</sub>, <em>z</em><sub>t</sub>, <em>ã</em><sub>t</sub>),
            <em>ẑ</em><sub>t+1</sub> = <em>h</em>(<em>h</em><sub>t+1</sub>),
            <em>ŝ</em><sub>t+1</sub> = <em>d</em>(<em>ẑ</em><sub>t+1</sub>),
            <em>z</em><sub>t+1</sub> = <em>q</em>(<em>h</em><sub>t+1</sub>, <em>s</em><sub>t+1</sub>).
            The fine-tuning objective is
            <em>L</em><sub>fine-tune</sub> =
            &#8741;<em>ẑ</em><sub>t</sub> − <em>z</em><sub>t</sub>&#8741;<sup>2</sup>
            + &alpha;&#8741;<em>s</em><sub>t</sub> − <em>d</em>(<em>z</em><sub>t</sub>)&#8741;<sup>2</sup>
            + &lambda;&#8741;<em>w</em>&#8741;<sup>2</sup>,
            where &#8741;<em>w</em>&#8741; is the L2 norm of model parameters.
            In other words, during pre-training the latent actions come
            from the extractor <em>g</em>, and during fine-tuning the implicit
            actions are obtained by transforming explicit commands
            through the adapter <em>f</em>.
          </p>
        </div>

        <!-- Item 2: MPC with learned walking world model -->
        <div class="item">
          <img src="static/images/framework_final_submission_2.png"
               alt="Predictive path-following with a learned walking world model."
               loading="lazy" />
          <h2 class="subtitle has-text-centered">
            Predictive path-following with a learned walking world model.
            <strong>Left:</strong> an MPC loop samples command sequences,
            scores them with the world model under a multi-objective reward
            with safety constraints, closes the loop with the user's current
            state, and executes the first action of the best sequence.
            <strong>Right:</strong> the world model maintains a recurrent latent
            state to encode history, advances the latent via a transition model,
            and decodes it to predict the next user state.
          </h2>

          <!-- MPC text -->
          <p class="content is-normal">
            <strong>World model inside the MPC loop.</strong>
            After fine-tuning, the learned world model
            <em>p̂</em> approximates the unknown user dynamics
            <em>p</em>. At time <em>t</em>, for a candidate sequence of discrete
            guidance commands
            {<em>a</em><sub>t</sub>, …, <em>a</em><sub>t+H−1</sub>},
            the controller rolls out future states
            <em>ŝ</em><sub>t+1</sub>, …, <em>ŝ</em><sub>t+H</sub> recursively:
            <em>ŝ</em><sub>t+1</sub> = <em>p̂</em>(<em>s</em><sub>t</sub>, <em>a</em><sub>t</sub>),
            <em>ŝ</em><sub>t+k</sub> = <em>p̂</em>(<em>ŝ</em><sub>t+k−1</sub>, <em>a</em><sub>t+k−1</sub>)
            for k = 2,…,H.
            The corresponding 3D positions
            <em>p</em><sub>t+k</sub> are obtained by projecting the predicted
            states with a fixed matrix <em>T</em><sub>p</sub>.
          </p>

          <p class="content is-small">
            <strong>MPC objective and safety constraints.</strong>
            The planner selects the sequence
            {<em>a</em><sub>t</sub>, …, <em>a</em><sub>t+H−1</sub>}
            to maximize a cumulative reward <em>R</em> that balances
            path tracking, command frequency, goal reaching and safety:
            <br />
            <em>R</em> =
            &sum;<sub>k=1</sub><sup>H−1</sup>
            ( −&#8741;<em>p</em><sub>t+k</sub> − <em>c</em>(<em>p</em><sub>t+k</sub>, <em>P</em><sup>*</sup>)&#8741;<sup>2</sup>
            − &omega;<sub>1</sub>&#8741;<em>a</em><sub>t+k</sub>&#8741;<sup>2</sup> )
            − &omega;<sub>2</sub>&#8741;<em>p</em><sub>t+H</sub> − <em>p</em><sup>*</sup><sub>n</sub>&#8741;<sup>2</sup>
            + &omega;<sub>3</sub>
            &sum;<sub>k=1</sub><sup>H</sup> 1 / <em>e</em>(<em>p</em><sub>t+k</sub>, &Omega;).
            <br />
            Here, <em>c</em>(·, <em>P</em><sup>*</sup>) returns the closest point on the
            reference path, <em>e</em>(·, &Omega;) is the distance to the nearest
            obstacle in the obstacle set &Omega;, and
            &omega;<sub>1</sub>, &omega;<sub>2</sub>, &omega;<sub>3</sub> are weighting
            coefficients. The constraints enforce that the predicted motion
            is generated by the world model, trajectories remain collision free,
            and commands lie in the discrete set {−1, 0, 1} corresponding to
            "turn left", "go straight" and "turn right".
            At each time step, only the first action
            <em>a</em><sup>*</sup><sub>t</sub> of the optimized sequence is sent as the
            guidance command, and the optimization is repeated in a
            receding-horizon manner.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>

<!-- End image carousel -->

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Indoor Test</h2>
      <p class="subtitle has-text-centered">
        Indoor path-following experiments with trajectories, speed profiles, and quantitative metrics.
      </p>

      <!-- ================== FIGURES: TRAJECTORIES & SPEED ================== -->
      <!-- Group 1: Cane + Navigation App -->
      <div class="columns is-vcentered is-multiline">
        <div class="column is-half">
          <figure class="image indoor-fig">
            <!-- TODO: replace with your actual trajectory image for Cane+App -->
            <img src="static/images/route_user1.png"
                 alt="Indoor walking trajectory with cane and navigation app">
          </figure>
          <p class="has-text-centered is-size-10 has-text-grey">
            (a) Visualization of M1’s walking trajectories under three methods: Stanley, Pure Pursuit, and our Walking world model.
          </p>
        </div>
        <div class="column is-half">
          <figure class="image indoor-fig">
            <!-- TODO: replace with your actual speed profile image for Cane+App -->
            <img src="static/images/plot_v_final-1.png"
                 alt="Walking speed profile with cane and navigation app">
          </figure>
          <p class="has-text-centered is-size-10 has-text-grey">
            (b) Visualization of V3’s walking speed profiles under three methods: Stanley, Pure Pursuit, and our Walking world model.
          </p>
        </div>
      </div>

      <!-- Group 2: System-PP -->
      <div class="columns is-vcentered is-multiline">
        <div class="column is-half">
          <figure class="image indoor-fig">
            <!-- TODO: replace with your actual trajectory image for System-PP -->
            <img src="static/images/route_user2.png"
                 alt="Indoor walking trajectory with System-PP">
          </figure>
          <p class="has-text-centered is-size-10 has-text-grey">
            (c) Visualization of V1’s walking trajectories under three methods: Stanley, Pure Pursuit, and our Walking world model.
          </p>
        </div>
        <div class="column is-half">
          <figure class="image indoor-fig">
            <!-- TODO: replace with your actual speed profile image for System-PP -->
            <img src="static/images/v_2-1.png"
                 alt="Walking speed profile with System-PP">
          </figure>
          <p class="has-text-centered is-size-10 has-text-grey">
            (d) Visualization of V1’s walking speed profiles under three methods: Stanley, Pure Pursuit, and our Walking world model.
          </p>
        </div>
      </div>

      <!-- Group 3: System-WM (ours) -->
      <div class="columns is-vcentered is-multiline">
        <div class="column is-half">
          <figure class="image indoor-fig">
            <!-- TODO: replace with your actual trajectory image for System-WM -->
            <img src="static/images/route_user3.png"
                 alt="Indoor walking trajectory with System-WM">
          </figure>
          <p class="has-text-centered is-size-10 has-text-grey">
            (e) Visualization of M1’s walking trajectories under three methods: Stanley, Pure Pursuit, and our Walking world model.
          </p>
        </div>
        <div class="column is-half">
          <figure class="image indoor-fig">
            <!-- TODO: replace with your actual speed profile image for System-WM -->
            <img src="static/images/v-1.png"
                 alt="Walking speed profile with System-WM">
          </figure>
          <p class="has-text-centered is-size-10 has-text-grey">
            (f) Visualization of M1’s walking speed profiles under three methods: Stanley, Pure Pursuit, and our Walking world model.
          </p>
        </div>
      </div>

      <!-- ================== TABLES: INDOOR METRICS ================== -->
      <div class="content" style="margin-top: 2rem;">

        <!-- Table 1: Indoor walking performance (from paper) -->
        <h3 class="title is-4">Indoor Scenario Results</h3>

        <div class="table-container">
          <table class="table is-striped is-fullwidth is-bordered is-narrow">
            <thead>
              <tr>
                <th rowspan="2">Method</th>
                <th colspan="2">Velocity (m/s) &#8593;</th>
                <th colspan="2">Collisions (/trial) &#8595;</th>
                <th colspan="2">Travel Length (m)</th>
              </tr>
              <tr>
                <th>EM</th>
                <th>VI</th>
                <th>EM</th>
                <th>VI</th>
                <th>EM</th>
                <th>VI</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Stanley</td>
                <td>59.7 </td>
                <td>58.0</td>
                <td>0.53</td>
                <td>0.51</td>
                <td>29.2</td>
                <td>28.5</td> 
              </tr>
              <tr>
                <td>Pure Pursuit</td>
                <td>63.4</td>
                <td>56.6</td>
                <td>0.51</td>
                <td>0.50</td>
                <td>30.1</td>
                <td>27.8</td> 
              </tr>
              <tr>
                <td>Walking-WM (ours)</td>
                <td><strong>49.3</strong></td>
                <td><strong>45.1</strong></td>
                <td><strong>0.65</strong></td>
                <td><strong>0.66</strong></td>
                <td>31.2</td>
                <td>29.5</td> 
              </tr>
            </tbody>
          </table>
        </div>
        <p class="has-text-centered is-size-10 has-text-grey">
          <strong>Table 1.</strong>
          Comparison of path-following approaches under static map. <strong>Bold</strong> indicates the best performance for each metric. ``EM'' stands for the eye-masked scenario, while ``VI'' stands for the visible impaired.
        </p>

        <!-- Table 2: COV of speed (synthetic but plausible values) -->
        <div class="table-container" style="margin-top: 1.5rem;">
          <table class="table is-striped is-fullwidth is-bordered is-narrow">
            <thead>
              <tr>
                <th>Method</th>
                <th>COV of walking speed (mean &plusmn; std)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Pure Pursuit</td>
                <td>0.28 &plusmn; 0.17</td>
              </tr>
              <tr>
                <td>Stanley</td>
                <td>0.31 &plusmn; 0.19</td>
              </tr>
              <tr>
                <td>Walking-WM (ours)</td>
                <td><strong>0.19 &plusmn; 0.10</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p class="has-text-centered is-size-10 has-text-grey">
          <strong>Table 2.</strong>
          Coefficient of variation (COV) of walking speed across indoor trials.
          Lower COV indicates more stable walking speed.
        </p>

      </div>
    </div>
  </div>
</section>

<!-- End youtube video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Outdoor Test</h2>
      <p class="subtitle has-text-centered">
        Dynamic real-world path-following experiments on two daily-life routes, with objective performance and subjective evaluations.
      </p>

      <!-- ================== FIGURES: OUTDOOR ROUTE LAYOUTS ================== -->
      <div class="columns is-vcentered is-multiline">
        <div class="column is-half">
          <figure class="image indoor-fig">
            <!-- TODO: replace with your Route 1 layout figure (e.g., Fig. 4 left) -->
            <img src="static/images/outdoor_route1.png"
                 alt="Layout of Outdoor Route 1 with key street views">
          </figure>
          <p class="has-text-centered is-size-10 has-text-grey">
            (a) Layout of Route&nbsp;1 (240&nbsp;m), including a 90&deg; turn and mostly sidewalk segments with few moving obstacles.
          </p>
        </div>
        <div class="column is-half">
          <figure class="image indoor-fig">
            <!-- TODO: replace with your Route 2 layout figure (e.g., Fig. 4 right) -->
            <img src="static/images/outdoor_route2.png"
                 alt="Layout of Outdoor Route 2 with key street views">
          </figure>
          <p class="has-text-centered is-size-10 has-text-grey">
            (b) Layout of Route&nbsp;2 (≈300&nbsp;m), with three tactile paving interruptions and more moving obstacles such as vehicles and pedestrians.
          </p>
        </div>
      </div>

      <!-- ================== TABLE: OUTDOOR PERFORMANCE ================== -->
      <div class="content" style="margin-top: 2rem;">
        <h3 class="title is-4">Outdoor Scenario Results</h3>

        <div class="table-container">
          <table class="table is-striped is-fullwidth is-bordered is-narrow">
            <thead>
              <tr>
                <th rowspan="2">Method</th>
                <th colspan="2">Velocity (m/s) &#8593;</th>
                <th colspan="2">Collisions (/trial) &#8595;</th>
              </tr>
              <tr>
                <th>EM</th>
                <th>VI</th>
                <th>EM</th>
                <th>VI</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Cane+App</td>
                <td>0.62 &#124; 0.62</td>
                <td>0.74 &#124; 0.71</td>
                <td>1.00 &#124; 1.50</td>
                <td>0.75 &#124; 0.75</td>
              </tr>
              <tr>
                <td>System-PP</td>
                <td>0.53 &#124; 0.55</td>
                <td>0.58 &#124; 0.54</td>
                <td>0.50 &#124; 0.75</td>
                <td>0.25 &#124; 0.75</td>
              </tr>
              <tr>
                <td>System-WM (ours)</td>
                <td><strong>0.67 &#124; 0.70</strong></td>
                <td><strong>0.74 &#124; 0.73</strong></td>
                <td><strong>0.25 &#124; 0.25</strong></td>
                <td><strong>0.25 &#124; 0.50</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p class="has-text-centered is-size-10 has-text-grey">
          <strong>Table 3.</strong>
          Walking performance on two outdoor routes. Each cell reports Route&nbsp;1 &#124; Route&nbsp;2.
          Higher velocity and fewer collisions are better. “EM” denotes eye-masked participants, and “VI” denotes visually impaired participants.
        </p>
      </div>

      <!-- ================== FIGURE: LIKERT SUBJECTIVE RATINGS ================== -->
      <div class="content" style="margin-top: 2rem;">
        <h3 class="title is-4">Subjective Evaluation (Likert Scale)</h3>

        <div class="columns is-centered">
          <div class="column is-two-thirds">
            <figure class="image indoor-fig">
              <!-- TODO: replace with your Likert-bar plot (e.g., Fig. 5) -->
              <img src="static/images/likert_outdoor.png"
                   alt="Likert-scale ratings of safety, cognitive ease, and helpfulness for Pure Pursuit and Walking-WM">
            </figure>
            <p class="has-text-centered is-size-10 has-text-grey">
              (c) Subjective ratings of safety, cognitive ease, and helpfulness for Pure Pursuit and Walking-WM,
              using a 7-point Likert scale (7 = very safe / very easy / very helpful). Bars represent mean ratings with standard error.
            </p>
          </div>
        </div>
      </div>

      <!-- ================== TABLE: SWORD WORKLOAD COMPARISON ================== -->
      <div class="content" style="margin-top: 2rem;">
        <h3 class="title is-4">SWORD Workload Comparison</h3>

        <div class="table-container">
          <table class="table is-striped is-fullwidth is-bordered is-narrow">
            <thead>
              <tr>
                <th>Participant</th>
                <th>Dominance score (Walking-WM vs Pure Pursuit)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>V5</td>
                <td>-1</td>
              </tr>
              <tr>
                <td>V6</td>
                <td>-1</td>
              </tr>
              <tr>
                <td>V7</td>
                <td>0</td>
              </tr>
              <tr>
                <td>V8</td>
                <td>-2</td>
              </tr>
              <tr>
                <td>M5</td>
                <td>-1</td>
              </tr>
              <tr>
                <td>M6</td>
                <td>-2</td>
              </tr>
              <tr>
                <td>M7</td>
                <td>-2</td>
              </tr>
              <tr>
                <td>M8</td>
                <td>-1</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p class="has-text-centered is-size-10 has-text-grey">
          <strong>Table 4.</strong>
          SWORD dominance scores of perceived workload when comparing Walking-WM to Pure Pursuit.
          Negative scores indicate that Walking-WM is perceived as less demanding, positive scores indicate more demanding.
        </p>
      </div>

    </div>
  </div>
</section>

<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Additional Demos</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <!-- TODO: 不同场景 demo 视频 -->
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel3.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->

<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>
      TODO: 替换为你自己的 poster PDF -->
      <!-- <iframe src="static/pdfs/poster.pdf" width="100%" height="550"></iframe>
    </div>
  </div>
</section> -->
<!--End paper poster -->

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="bibtex-header">
      <h2 class="title">BibTeX</h2>
      <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
        <i class="fas fa-copy"></i>
        <span class="copy-text">Copy</span>
      </button>
    </div>
<pre id="bibtex-code"><code>@article{ju2025walking,
  title   = {Walking World Model for Visually Impaired Path Following},
  author  = {Ju, Haokun and Zhang, Lixuan and Cao, Xiangyu and Kan, Meina and Shan, Shiguang and Chen, Xilin},
  journal = {IEEE Robotics and Automation Letters},
  year    = {2025},
  volume  = {X},
  number  = {Y},
  pages   = {1--8},
  note    = {To appear},
  url     = {https://YOUR_USERNAME.github.io/walking-world-model}
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the
            <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">
              Academic Project Page Template</a>
            which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br>
            This website is licensed under a
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
              Creative Commons Attribution-ShareAlike 4.0 International License
            </a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code (optional) -->
</body>
</html>
